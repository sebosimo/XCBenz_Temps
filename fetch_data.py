import os, sys
import datetime, json, xarray as xr
import numpy as np
import warnings
import requests
import time
import shutil

# Set GRIB definitions for COSMO/ICON
COSMO_DEFS = r"C:\Users\sebas\.conda\envs\weather_final\share\eccodes-cosmo-resources\definitions"
STANDARD_DEFS = os.path.join(sys.prefix, "Library", "share", "eccodes", "definitions")

defs_to_use = []
if os.path.exists(COSMO_DEFS):
    defs_to_use.append(COSMO_DEFS)
if os.path.exists(STANDARD_DEFS):
    defs_to_use.append(STANDARD_DEFS)

if defs_to_use:
    final_def_path = ":".join(defs_to_use)
    os.environ["GRIB_DEFINITION_PATH"] = final_def_path
    os.environ["ECCODES_DEFINITION_PATH"] = final_def_path

from meteodatalab import ogd_api

# Suppress warnings
warnings.filterwarnings("ignore")

# --- Configuration ---
VARS_TRACES = ["T", "U", "V", "P", "QV"]
VARS_MAPS = ["U", "V", "HHL"] 
CACHE_DIR_TRACES = "cache_data"
CACHE_DIR_MAPS = "cache_wind"
STATIC_DIR = "static_data"
HHL_FILENAME = "vertical_constants_icon-ch1-eps.grib2"
HGRID_FILENAME = "horizontal_constants_icon-ch1-eps.grib2"
STAC_BASE_URL = "https://data.geo.admin.ch/api/stac/v1/collections/ch.meteoschweiz.ogd-forecasting-icon-ch1"
STAC_ASSETS_URL = f"{STAC_BASE_URL}/assets"

WIND_LEVELS = []

os.makedirs(CACHE_DIR_TRACES, exist_ok=True)
os.makedirs(CACHE_DIR_MAPS, exist_ok=True)

def get_iso_horizon(total_hours):
    days = total_hours // 24
    hours = total_hours % 24
    return f"P{days}DT{hours}H"

def sanitize_name(name):
    n = name.replace("ü", "ue").replace("ö", "oe").replace("ä", "ae") \
            .replace("Ü", "Ue").replace("Ö", "Oe").replace("Ä", "Ae").replace("ß", "ss")
    clean = "".join(c for c in n if c.isalnum() or c in ('-', '_'))
    return clean if clean else "unnamed"

def log(msg, level="INFO"):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open("debug_log.txt", "a") as f:
        f.write(f"{timestamp} [{level}] {msg}\n")
    print(f"{timestamp} [{level}] {msg}", flush=True)

def download_file(url, target_path, max_retries=3):
    """Downloads a file with retries and exponential backoff."""
    backoff = 2
    for attempt in range(max_retries):
        try:
            log(f"Downloading {url} to {target_path}...")
            with requests.get(url, stream=True, timeout=30) as r:
                r.raise_for_status()
                with open(target_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=1024*1024):
                        f.write(chunk)
            log(f"Download complete: {target_path}")
            return True
        except Exception as e:
            log(f"Download attempt {attempt+1} failed: {e}", "ERROR")
            if attempt < max_retries - 1:
                time.sleep(backoff ** attempt)
    return False

def get_latest_available_runs(limit=1):
    """Discovers actual runs available on the server using Active Probing."""
    log("Discovering runs via Active Probing...")
    now = datetime.datetime.now(datetime.timezone.utc)
    hour = (now.hour // 3) * 3
    start = now.replace(hour=hour, minute=0, second=0, microsecond=0)
    
    found_runs = []
    for i in range(16): # Check last 48 hours
        cand = start - datetime.timedelta(hours=i*3)
        ref = cand.strftime("%Y-%m-%dT%H:%M:%SZ")
        
        params = {"limit": 1, "forecast:reference_datetime": ref}
        try:
            r = requests.get(f"{STAC_BASE_URL}/items", params=params, timeout=10)
            if r.status_code == 200 and r.json().get("features"):
                found_runs.append(cand)
                log(f"Found available run: {ref}")
                if len(found_runs) >= limit: break
        except: pass
    return found_runs

def download_static_files():
    os.makedirs(STATIC_DIR, exist_ok=True)
    for filename in [HHL_FILENAME, HGRID_FILENAME]:
        path = os.path.join(STATIC_DIR, filename)
        if not os.path.exists(path):
            log(f"Downloading static file {filename}...")
            try:
                resp = requests.get(STAC_ASSETS_URL, timeout=10)
                assets = resp.json()["assets"]
                url = next((a["href"] for a in assets if a.get("id") == filename), None)
                if url: download_file(url, path)
            except Exception as e: log(f"Failed to fetch static {filename}: {e}", "ERROR")

def load_static_hhl():
    path = os.path.join(STATIC_DIR, HHL_FILENAME)
    if not os.path.exists(path): return None
    try:
        ds = xr.open_dataset(path, engine='cfgrib', backend_kwargs={'indexpath': ''})
        var = next((v for v in ds.data_vars if v.lower() in ['h', 'hhl']), list(ds.data_vars)[0])
        hhl = ds[var].load()
        ds.close()
        return hhl
    except Exception as e: log(f"Error loading HHL: {e}", "ERROR"); return None

def load_static_grid():
    path = os.path.join(STATIC_DIR, HGRID_FILENAME)
    if not os.path.exists(path): return None
    try:
        ds = xr.open_dataset(path, engine='cfgrib', backend_kwargs={'indexpath': ''})
        grid = {}
        for key in ['lat', 'lon']:
            # Search both coordinates and data variables
            match_k = next((k for k in list(ds.coords) + list(ds.data_vars) if key in k.lower()), None)
            if match_k:
                grid[key] = ds[match_k].load()
            else:
                grid[key] = None
        ds.close()
        return grid if grid.get('lat') is not None else None
    except Exception as e: log(f"Error loading HGRID: {e}", "ERROR"); return None

def is_run_complete_locally(time_tag, locations, max_h):
    last_loc = sanitize_name(list(locations.keys())[-1])
    trace_path = os.path.join(CACHE_DIR_TRACES, time_tag, last_loc, f"H{max_h:02d}.nc")
    map_path = os.path.join(CACHE_DIR_MAPS, time_tag, f"wind_maps_H{max_h:02d}.nc")
    return os.path.exists(trace_path) and os.path.exists(map_path)

def process_traces(fields, locations, tag, h, ref):
    sample = list(fields.values())[0]
    lat_n = 'latitude' if 'latitude' in sample.coords else 'lat'
    lon_n = 'longitude' if 'longitude' in sample.coords else 'lon'
    lats, lons = sample[lat_n].values, sample[lon_n].values
    indices = {n: int(np.argmin((lats-c['lat'])**2+(lons-c['lon'])**2)) for n, c in locations.items()}

    for name, idx in indices.items():
        # New Naming: [Location]_[RunTag]_H[horizon].nc
        safe_name = sanitize_name(name)
        loc_dir = os.path.join(CACHE_DIR_TRACES, tag, safe_name)
        os.makedirs(loc_dir, exist_ok=True)
        filename = f"H{h:02d}.nc"
        path = os.path.join(loc_dir, filename)
        
        if os.path.exists(path): continue

        loc_vars = {}
        hhl_profile = None
        
        # 1. First, check if HHL is available to determine the target level count
        if "HHL" in fields:
            ds_hhl = fields["HHL"]
            s_dim = ds_hhl[lat_n].dims[0]
            z_dim = ds_hhl.dims[0] # Usually the vertical dimension
            hhl_profile = ds_hhl.squeeze().isel({s_dim: idx}).compute()
            # Calculate cell-center heights (80 values from 81 half-levels)
            h_vals = hhl_profile.values
            height_centers = (h_vals[:-1] + h_vals[1:]) / 2.0
            
            # Create a DataArray for HEIGHT
            loc_vars["HEIGHT"] = xr.DataArray(height_centers, dims=["level"])

        # 2. Process all other variables
        for var, ds in fields.items():
            if var == "HHL": continue # Skip raw HHL
            
            s_dim = ds[lat_n].dims[0]
            profile = ds.squeeze().isel({s_dim: idx}).compute()
            if profile.dims: 
                # Rename the vertical dimension to 'level' for consistency
                profile = profile.rename({profile.dims[0]: 'level'})
            
            # Drop unnecessary coordinates but keep the data
            loc_vars[var] = profile.drop_vars([c for c in profile.coords if c not in profile.dims])

        ds_out = xr.Dataset(loc_vars)
        valid_time = ref + datetime.timedelta(hours=h)
        ds_out.attrs = {
            "location": name, 
            "ref_time": ref.isoformat(), 
            "horizon": h,
            "valid_time": valid_time.isoformat()
        }
        ds_out.to_netcdf(path)

def process_wind_maps(fields, tag, h_int, ref):
    if "U" not in fields or "V" not in fields or "HHL" not in fields:
        missing = [k for k in ["U", "V", "HHL"] if k not in fields]
        log(f"process_wind_maps aborting. Missing fields: {missing}", "ERROR")
        return
    
    # Load WIND_LEVELS from JSON if not already loaded available globally or passed
    # For now assuming global WIND_LEVELS is populated in main/config
    
    from metpy.interpolate import interpolate_to_isosurface
    u, v, hhl = fields["U"].squeeze(), fields["V"].squeeze(), fields["HHL"].squeeze()
    
    try:
        z_dim = hhl.dims[0]
        z_f = (hhl.isel({z_dim: slice(0,-1)}).values + hhl.isel({z_dim: slice(1,None)}).values) / 2
        h_surf = hhl.isel({z_dim: -1})
        
        np_u, np_v = u.values, v.values
        np_z = z_f
        
        for lvl in WIND_LEVELS:
            try:
                # New Naming: Wind_[Type]_[Level]_[RunTag]_H[horizon].nc
                fname = f"Wind_{lvl['type']}_{lvl['name']}_{tag}_H{h_int:02d}.nc"
                out_dir = os.path.join(CACHE_DIR_MAPS, tag)
                os.makedirs(out_dir, exist_ok=True)
                output_path = os.path.join(out_dir, fname)
                
                if os.path.exists(output_path): continue

                target_z = np_z - h_surf.values if lvl['type'] == 'AGL' else np_z
                res_u = interpolate_to_isosurface(target_z, np_u, lvl['h'])
                res_v = interpolate_to_isosurface(target_z, np_v, lvl['h'])
                
                spatial = u.dims[-1]
                coords = {spatial: u[spatial], "latitude": u.latitude, "longitude": u.longitude}
                
                out_ds = xr.Dataset({
                    f"u_{lvl['name']}": xr.DataArray(res_u, dims=[spatial], coords=coords),
                    f"v_{lvl['name']}": xr.DataArray(res_v, dims=[spatial], coords=coords)
                })
                valid_time = ref + datetime.timedelta(hours=h_int)
                out_ds.attrs = {
                    "level_name": lvl['name'], 
                    "level_type": lvl['type'], 
                    "level_h": lvl['h'], 
                    "ref_time": ref.isoformat(),
                    "horizon": h_int,
                    "valid_time": valid_time.isoformat()
                }
                out_ds.to_netcdf(output_path)
                log(f"Saved wind map: {fname}")
            except Exception as e: log(f"Error processing level {lvl['name']}: {e}", "ERROR")

    except Exception as e: log(f"Wind map setup error: {e}", "ERROR")

def main():
    log("Main start...")
    
    # Load WIND_LEVELS from JSON
    global WIND_LEVELS
    try:
        if os.path.exists("wind_levels.json"):
            with open("wind_levels.json", "r") as f:
                WIND_LEVELS = json.load(f)
            log(f"Loaded {len(WIND_LEVELS)} wind levels from config.")
        else:
            log("Warning: wind_levels.json not found! Wind maps will be skipped.", "WARNING")
    except Exception as e:
        log(f"Error loading wind_levels.json: {e}", "ERROR")

    cleanup_old_runs() # Cleanup BEFORE downloading to free space
    download_static_files()
    if not os.path.exists("locations.json"): return
    with open("locations.json", "r", encoding="utf-8") as f: locations = json.load(f)

    runs = get_latest_available_runs(limit=3)
    if not runs: log("No runs found."); return

    hhl = load_static_hhl()
    grid = load_static_grid()

    if hhl is not None and grid is not None:
        # Inject coords into HHL so it can serve as a sample for process_traces
        n_grid = grid['lat'].size
        match_dim = next((d for d in hhl.dims if hhl.sizes[d] == n_grid), None)
        if match_dim:
            hhl = hhl.assign_coords({
                "latitude": (match_dim, grid['lat'].values),
                "longitude": (match_dim, grid['lon'].values)
            })

    for ref_time in runs:
        tag = ref_time.strftime('%Y%m%d_%H%M')
        max_h = 45 if ref_time.hour == 3 else 33
        if is_run_complete_locally(tag, locations, max_h):
            log(f"Run {tag} complete locally."); break
        
        log(f"Processing run: {tag}")
        any_success = False
        for h in range(max_h + 1):
            iso_h = get_iso_horizon(h)
            valid_time_str = (ref_time + datetime.timedelta(hours=h)).strftime('%Y-%m-%dT%H:%M:%SZ')
            # Only log detailed info if we actually have chance of finding data
            
            fields = {"HHL": hhl} if hhl is not None else {}
            has_new_data = False
            for var in ["T", "U", "V", "P", "QV"]:
                try:
                    req = ogd_api.Request(collection="ogd-forecasting-icon-ch1", variable=var,
                                         reference_datetime=ref_time, horizon=iso_h, perturbed=False)
                    urls = ogd_api.get_asset_urls(req)
                    if urls:
                        tmp = f"temp_{var}_{tag}_{h:02d}.grib2"
                        if download_file(urls[0], tmp):
                            ds = xr.open_dataset(tmp, engine='cfgrib', backend_kwargs={'indexpath': ''})
                            data = ds[next(iter(ds.data_vars))].load()
                            if grid:
                                m_dim = next(d for d in data.dims if data.sizes[d] == grid['lat'].size)
                                data = data.assign_coords({"latitude": (m_dim, grid['lat'].values), "longitude": (m_dim, grid['lon'].values)})
                            fields[var] = data
                            ds.close()
                            os.remove(tmp)
                            has_new_data = True
                except: pass
            
            if has_new_data:
                process_traces(fields, locations, tag, h, ref_time)
                # process_wind_maps(fields, tag, h, ref_time)
                log(f"H+{h:02d} done")
                any_success = True
        
        if any_success:
            log(f"Run {tag} processing complete.", "NOTICE")
            break # Success, don't Fallback to older runs
        else:
            log(f"Run {tag} yield no data, trying next available run...")
            # Cleanup the empty directory if it was created
            for d in [CACHE_DIR_TRACES, CACHE_DIR_MAPS]:
                p = os.path.join(d, tag)
                if os.path.exists(p) and not os.listdir(p):
                    try: shutil.rmtree(p)
                    except: pass

    generate_manifest()
    log("--- Data Fetcher Complete ---", "NOTICE")

def generate_manifest():
    """Write manifest.json reflecting current cache_data contents."""
    runs = {}
    if os.path.exists(CACHE_DIR_TRACES):
        for run in sorted(os.listdir(CACHE_DIR_TRACES), reverse=True):
            run_path = os.path.join(CACHE_DIR_TRACES, run)
            if not os.path.isdir(run_path):
                continue
            locations = {}
            for loc in sorted(os.listdir(run_path)):
                loc_path = os.path.join(run_path, loc)
                if not os.path.isdir(loc_path):
                    continue
                steps = sorted(
                    f.replace(".nc", "")
                    for f in os.listdir(loc_path)
                    if f.endswith(".nc")
                )
                if steps:
                    locations[loc] = steps
            if locations:
                runs[run] = locations

    manifest = {
        "generated_at": max(runs.keys()) if runs else "",
        "runs": runs,
    }
    with open("manifest.json", "w") as f:
        json.dump(manifest, f)
    log(f"Manifest written: {len(runs)} runs")

def cleanup_old_runs():
    ret = os.environ.get("RETENTION_DAYS")
    if not ret: return
    cutoff = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=int(ret))
    for d in [CACHE_DIR_TRACES, CACHE_DIR_MAPS]:
        if not os.path.exists(d): continue
        for item in os.listdir(d):
            path = os.path.join(d, item)
            try:
                dt = datetime.datetime.strptime(item, "%Y%m%d_%H%M").replace(tzinfo=datetime.timezone.utc)
                if dt < cutoff: shutil.rmtree(path)
            except: pass

if __name__ == "__main__":
    main()
